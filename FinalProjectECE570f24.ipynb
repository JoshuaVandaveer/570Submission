{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C9FftPPDpdO"
      },
      "source": [
        "# Efficient Group Equivairent Network\n",
        "This notebook contains the code for creating and comparing an efficient group equivarient CNN.\n",
        "The architecture of this network is adapted from the NeurIPS-2021 publication of Lingshen He, Yuxuan Chen, Zhengyang Shen, Yiming Dong, Yisen Wang, and Zhouchen Lin. Adaptions and improvments will be noted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWLGyMPDE840"
      },
      "outputs": [],
      "source": [
        "# import all necessary tools\n",
        "# import libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZHO6kZ1qcIY",
        "outputId": "ac3e252b-3fbc-4c7f-9ed9-d1a3dd1932bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjFM4rwuFY6r"
      },
      "source": [
        "## Experiment\n",
        "This experiment will compare the Cross Entropy Loss and final classification accuracy of 4 neural networks. Each network will trained on the special Orthogonal groups (SO2) of the CIFAR and MNIST datasets.\n",
        "\n",
        "The four neural networks are\n",
        "1.  **Traditional CNN.**\n",
        "  \n",
        "    A traditional CNN can achieve group equivarience by rotating the input across the whole group and maintaining the same training label.\n",
        "\n",
        "2.  **Group Equivarient CNN.**\n",
        "\n",
        "    The group equivarient CNN (G-CNN) will be created by using the approach proposed by He et. al. in the NeurIPS-2021 submission \"Efficient Equivarient Network\".\n",
        "3.  **Group Equivarient CNN with Equivarient MaxPooling.**\n",
        "\n",
        "    Equivarient max pooling layers will be created and added using the approch proposed by Xu et.al in the NeurIPS-2021 submission \"Group Equivarient Subsampling\".\n",
        "\n",
        "4.  **Group Equivarient CNN with Equivarient Maxpooling and non-equivarient attention**.\n",
        "\n",
        "    Adding non-equivarent attention encoders allows the network to learn the non-equivarient features of the input as well. This is especially important as the output may depend on the relative rotation of the detected features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAwDEbF2NXLc"
      },
      "source": [
        "## SO2 Group\n",
        "Create a class for applying the SO2 tansformations on input images to test and train our models\n",
        "Our S02 group will consist of all 90 degree rotations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-yIdsgqOYK4"
      },
      "outputs": [],
      "source": [
        "def s02_r4(x):\n",
        "  b, c, h, w = x.shape\n",
        "  elements = torch.zeros((b, 4, c, h, w))\n",
        "  # first element is the original all others are rotated by 90 degress along the height/width dim (2,3)\n",
        "  elements[::, 0, ...] = x\n",
        "  elements[::, 1, ...] = torch.rot90(x, 1, [2,3])\n",
        "  elements[::, 2, ...] = torch.rot90(x, 2, [2,3])\n",
        "  elements[::, 3, ...] = torch.rot90(x, 3, [2,3])\n",
        "  return elements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wrm8sx7VuKhT",
        "outputId": "bbf9cdc9-3854-439f-f677-49fa0b64f267"
      },
      "outputs": [],
      "source": [
        "# create the cifar10 datsets, transforms, group elements, and plot\n",
        "class cifar():\n",
        "  def __init__(self):\n",
        "    self.mean = np.array([0.49139968, 0.48215827 ,0.44653124])\n",
        "    self.std = np.array([0.24703233, 0.24348505, 0.26158768])\n",
        "    self.t = T.Compose( [T.ToTensor(),    # convert images to tensor form (pushes channel dimentions to beginning)\n",
        "             T.Normalize( self.mean,      # normalize with the known mean\n",
        "                          self.std) ])    # normalize with the known standard deviation\n",
        "    self.inv_t = T.Normalize(mean=(-self.mean/self.std), std=(1/self.std))\n",
        "\n",
        "    # set the batch size for training\n",
        "    self.batch_size = 4\n",
        "\n",
        "    # load the training set and create a data loader for the training set\n",
        "    self.train_data = torchvision.datasets.CIFAR10(root='./data', download=True,train=True, transform=self.t)\n",
        "    self.train_loader = torch.utils.data.DataLoader(self.train_data, batch_size=self.batch_size,\n",
        "                                              shuffle=True, num_workers=2)\n",
        "\n",
        "    self.test_data = torchvision.datasets.CIFAR10(root='./data', download=True, transform=self.t, train=False)\n",
        "    self.test_loader = torch.utils.data.DataLoader(self.test_data, batch_size=self.batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    self.classes = ( 'plane', 'car', 'bird', 'cat', 'deer',\n",
        "            'dog', 'frog', 'horse', 'ship', 'truck' )\n",
        "\n",
        "  def transform_image(self, img):\n",
        "    img = self.inv_t(img)    # unormalize\n",
        "    img = img.numpy()\n",
        "    img = np.transpose(img, (1,2,0))\n",
        "    return img\n",
        "\n",
        "  def plot_rotations(self, images):\n",
        "    fig, axes = plt.subplots(self.batch_size, 4, figsize=(8,8))\n",
        "    for i  in range(self.batch_size):\n",
        "      for j in range(4):\n",
        "        image = images[i, j]\n",
        "        image = self.transform_image(image)\n",
        "        ax = axes[i, j]\n",
        "        ax.imshow(image)\n",
        "        ax.axis('off')\n",
        "\n",
        "\n",
        "\n",
        "Ciphar = cifar()\n",
        "print(f\"Training size is {len(Ciphar.train_loader)}\")\n",
        "print(f\"Testing size is {len(Ciphar.test_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "id": "1wVKHe2YP2Vc",
        "outputId": "0d2d5927-732d-40ba-ec33-8d58cc6c06e8"
      },
      "outputs": [],
      "source": [
        "dataiter = iter(Ciphar.train_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "print(f\"Images before rotation: {images.shape}\")\n",
        "images = s02_r4(images)\n",
        "print(f\"Images after rotation: {images.shape}\")\n",
        "\n",
        "Ciphar.plot_rotations(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LY-mg5Y2Jf3q",
        "outputId": "ed6c3fae-47ab-43ae-ddce-3352c6d48677"
      },
      "outputs": [],
      "source": [
        "class mnist():\n",
        "  def __init__(self):\n",
        "    self.transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
        "                      torchvision.transforms.Normalize((0.1307,),(0.3081,))])\n",
        "\n",
        "    self.train_dataset = torchvision.datasets.MNIST('data', train=True, download=True, transform=self.transform)\n",
        "    self.test_dataset = torchvision.datasets.MNIST('data', train=False, download=True, transform=self.transform)\n",
        "\n",
        "    self.batch_size = 4\n",
        "    self.train_loader = torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "    self.test_loader = torch.utils.data.DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "  def transform_image(self, img):\n",
        "    img = img[0]\n",
        "    return img\n",
        "\n",
        "  def plot_rotations(self, images):\n",
        "    fig, axes = plt.subplots(self.batch_size, 4, figsize=(8,8))\n",
        "    for i  in range(self.batch_size):\n",
        "      for j in range(4):\n",
        "        image = images[i, j]\n",
        "        image = self.transform_image(image)\n",
        "        ax = axes[i, j]\n",
        "        ax.imshow(image)\n",
        "        ax.axis('off')\n",
        "\n",
        "\n",
        "Mnist = mnist()\n",
        "print(f\"Training size is {len(Mnist.train_loader)}\")\n",
        "print(f\"Testing size is {len(Mnist.test_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "dY_Bjh97R6eQ",
        "outputId": "09dd6ef9-73af-4e84-c52e-6b6375830840"
      },
      "outputs": [],
      "source": [
        "dataiter = iter(Mnist.train_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "print(f\"Images before rotation: {images.shape}\")\n",
        "images = s02_r4(images)\n",
        "print(f\"Images after rotation: {images.shape}\")\n",
        "\n",
        "Mnist.plot_rotations(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N2-QCQjlO0tF",
        "outputId": "80bc4631-4eea-4a86-de01-65cef7b17d9c"
      },
      "outputs": [],
      "source": [
        "# test expansion of the rotations into the batch size\n",
        "dataiter = iter(Mnist.train_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "print(f\"Images before rotation: {images.shape}\")\n",
        "images = s02_r4(images)\n",
        "print(f\"Images after rotation: {images.shape}\")\n",
        "\n",
        "b, r, c, h, w = images.shape\n",
        "images = images.reshape(-1, c, h, w)\n",
        "labels = np.repeat(labels,r)\n",
        "\n",
        "print(f\"Images after expansion into batch dimention: {images.shape}\")\n",
        "print(f\"Labels after expansion into for rotations: {labels.shape}\")\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(images.shape[0], figsize=(15,15))\n",
        "for i in range(images.shape[0]):\n",
        "  plt.imshow(images[i][0])\n",
        "  ax = axes[i]\n",
        "  ax.imshow(images[i][0])\n",
        "  ax.set_title(f\"Label: {labels[i]}\")\n",
        "  ax.axis('off')\n",
        "\n",
        "plt.subplots_adjust(wspace=0.4, hspace=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKqD-KHbC4_F"
      },
      "outputs": [],
      "source": [
        "# train the model and print the error periodically\n",
        "class Trainer():\n",
        "  def __init__(self, model, criterion, optimizer, train_loader, test_loader):\n",
        "    self.model = model\n",
        "    self.criterion = criterion\n",
        "    self.optimizer = optimizer\n",
        "    self.train_loader = train_loader\n",
        "    self.test_loader = test_loader\n",
        "\n",
        "  def train(self, epochs):\n",
        "    self.model.train() # we need to set the mode for our model\n",
        "    total_loss = 0\n",
        "\n",
        "    len_loader = len(self.train_loader.dataset)\n",
        "    for e in range(epochs):\n",
        "      for batch_idx, (images, targets) in enumerate(self.train_loader):\n",
        "        images, targets = images.to(device), targets.to(device)\n",
        "        self.optimizer.zero_grad()\n",
        "        output = self.model(images)\n",
        "        loss = self.criterion(output, targets) # Here is a typical loss function (negative log likelihood)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch_idx % 200 == 0: # We visulize our output every 2000 batches\n",
        "          print(f'Epoch {e}: [{batch_idx*len(images)}/{len_loader}] Loss: {total_loss/200}')\n",
        "          total_loss = 0.0\n",
        "    print(\"Finished training\")\n",
        "\n",
        "  def train_c4(self, epochs):\n",
        "    self.model.to(device)\n",
        "    self.model.train() # we need to set the mode for our model\n",
        "    total_loss = 0\n",
        "\n",
        "    len_loader = len(self.train_loader.dataset)\n",
        "    for e in range(epochs):\n",
        "      for batch_idx, (images, targets) in enumerate(self.train_loader):\n",
        "        images, targets = images.to(device), targets.to(device)\n",
        "        b, c, h, w = images.shape\n",
        "        images = s02_r4(images).reshape(-1, c,h, w).to(device)\n",
        "        targets = targets.repeat(4).to(device)\n",
        "        self.optimizer.zero_grad()\n",
        "        output = self.model(images)\n",
        "        loss = self.criterion(output, targets) # Here is a typical loss function (negative log likelihood)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch_idx % 20 == 0: # We visulize our output every 10 batches\n",
        "          print(f'Epoch {e}: [{batch_idx*len(images)}/{len_loader}] Loss: {total_loss/20}')\n",
        "          total_loss = 0.0\n",
        "    print(\"Finished training\")\n",
        "\n",
        "  def test(self):\n",
        "    self.model.eval() # we need to set the mode for our model\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "      for images, targets in self.test_loader:\n",
        "        images, targets = images.to(device), targets.to(device)\n",
        "        output = self.model(images)\n",
        "        test_loss += self.criterion(output, targets).item()\n",
        "        pred = output.data.max(1, keepdim=True)[1] # we get the estimate of our result by look at the largest class value\n",
        "        correct += pred.eq(targets.data.view_as(pred)).sum() # sum up the corrected samples\n",
        "\n",
        "    test_loss /= len(self.test_loader.dataset)\n",
        "    print(f'Test: Avg loss is {test_loss}, Accuracy: {100.*correct/len(self.test_loader.dataset)}%')\n",
        "\n",
        "  def test_c4(self):\n",
        "    self.model.eval() # we need to set the mode for our model\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "      for images, targets in self.test_loader:\n",
        "        images, targets = images.to(device), targets.to(device)\n",
        "        b, c, h, w = images.shape\n",
        "        images = s02_r4(images).reshape(-1, c,h, w).to(device)\n",
        "        targets = targets.repeat(4).to(device)\n",
        "        output = self.model(images)\n",
        "        test_loss += self.criterion(output, targets).item()\n",
        "        pred = output.data.max(1, keepdim=True)[1] # we get the estimate of our result by look at the largest class value\n",
        "        correct += pred.eq(targets.data.view_as(pred)).sum() # sum up the corrected samples\n",
        "\n",
        "    test_loss /= len(self.test_loader.dataset)\n",
        "    print(f'Test C4: Avg loss is {test_loss}, Accuracy: {100.*correct/(len(self.test_loader.dataset)*4)}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASzts5VKKfN4"
      },
      "source": [
        "## Netork 1. Traditional CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TW8sp3i7DSjE"
      },
      "outputs": [],
      "source": [
        "class MyCNN(nn.Module):\n",
        "  def __init__(self, in_channels, in_h, in_w):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels, 6, 5)\n",
        "    size_after_conv1 = in_h - 5 + 1\n",
        "\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    size_after_max_pool1 = size_after_conv1 // 2 + (size_after_conv1 % 2)\n",
        "\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "    size_after_conv2 = size_after_max_pool1 - 5 + 1\n",
        "\n",
        "    size_after_max_pool2 = size_after_conv2 // 2 + (size_after_conv2 % 2)\n",
        "    self.fc1 = nn.Linear(16 *size_after_max_pool2  * size_after_max_pool2, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KHs7YWV2-xV"
      },
      "source": [
        "Train on a Baseline CNN on the MNIST and CIFAR Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVHM05VjCmjK",
        "outputId": "c8cdf4f8-8959-4995-de77-6a1c2e7619bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total parameters of BaselineCNN: 44426\n"
          ]
        }
      ],
      "source": [
        "# loss function\n",
        "import torch.optim as optim\n",
        "BaselineCNN = MyCNN(1, 28, 28).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(BaselineCNN.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# count the parameter\n",
        "par = sum(p.numel() for p in BaselineCNN.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters of BaselineCNN: {par}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKictnmfbfe0",
        "outputId": "50d29034-30e2-4b4e-ed6f-e0a8fdc9745a"
      },
      "outputs": [],
      "source": [
        "BaselineCNN_mnist = Trainer(BaselineCNN, criterion, optimizer, Mnist.train_loader, Mnist.test_loader)\n",
        "print(\"Baseline CNN no equivarience\")\n",
        "BaselineCNN_mnist.train(2)\n",
        "BaselineCNN_mnist.test()\n",
        "BaselineCNN_mnist.test_c4()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqTi-tDJikXX"
      },
      "outputs": [],
      "source": [
        "BaselineCNN = MyCNN(1, 28, 28).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(BaselineCNN.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xN660zG-fsHb",
        "outputId": "d31632ed-bdc0-492e-8d78-b30e8bda8303"
      },
      "outputs": [],
      "source": [
        "BaselineCNN_mnist = Trainer(BaselineCNN, criterion, optimizer, Mnist.train_loader, Mnist.test_loader)\n",
        "print(\"Baseline CNN with equivarience training\")\n",
        "BaselineCNN_mnist.train_c4(1)\n",
        "BaselineCNN_mnist.test()\n",
        "BaselineCNN_mnist.test_c4()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clT03KZY3HFf"
      },
      "source": [
        "Train the Baseline CNN on CIFAR dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UV1QI97b3KQC",
        "outputId": "9f404c55-bc62-49b8-f17a-d30276b934bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total parameters of BaselineCNN: 62006\n"
          ]
        }
      ],
      "source": [
        "# loss function\n",
        "import torch.optim as optim\n",
        "BaselineCNN = MyCNN(3, 32, 32).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(BaselineCNN.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# count the parameter\n",
        "par = sum(p.numel() for p in BaselineCNN.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters of BaselineCNN: {par}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4S1C8P53KY4",
        "outputId": "01387bae-2f3a-4339-803d-c0321d41c1c9"
      },
      "outputs": [],
      "source": [
        "BaselineCNN_cifar = Trainer(BaselineCNN, criterion, optimizer, Ciphar.train_loader, Ciphar.test_loader)\n",
        "print(\"Baseline CNN no equivarience\")\n",
        "BaselineCNN_cifar.train(1)\n",
        "BaselineCNN_cifar.test()\n",
        "BaselineCNN_cifar.test_c4()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4U8ean1S3Keo"
      },
      "outputs": [],
      "source": [
        "BaselineCNN = MyCNN(3, 32, 32).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(BaselineCNN.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOqqQz1_3Km7",
        "outputId": "11e79de8-f509-4ddc-ea9b-7a9277ad68c2"
      },
      "outputs": [],
      "source": [
        "BaselineCNN_cifar = Trainer(BaselineCNN, criterion, optimizer, Ciphar.train_loader, Ciphar.test_loader)\n",
        "print(\"Baseline CNN with equivarience training\")\n",
        "BaselineCNN_cifar.train_c4(1)\n",
        "BaselineCNN_cifar.test()\n",
        "BaselineCNN_cifar.test_c4()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiHmGH0-KlON"
      },
      "source": [
        "## Network 2. Efficient Group Equivarient CNN\n",
        "An efficient G-CNN can be created by the following steps\n",
        "\n",
        "\n",
        "1.   Lifting Convolution\n",
        "2.   G to G' mapping that reuses the same parameters\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpU6B-cCNFnT"
      },
      "outputs": [],
      "source": [
        "# encoder module\n",
        "\n",
        "##################################################\n",
        "class C_4_1x1(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(C_4_1x1, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        weight = torch.randn(out_channels, in_channels, 4) / math.sqrt(4 * in_channels / 2)\n",
        "        self.weight = torch.nn.Parameter(weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight = torch.zeros(self.out_channels, 4, self.in_channels, 4).to(x.device)\n",
        "        weight[::, 0, ...] = self.weight\n",
        "        weight[::, 1, ...] = self.weight[..., [3, 0, 1, 2]]\n",
        "        weight[::, 2, ...] = self.weight[..., [2, 3, 0, 1]]\n",
        "        weight[::, 3, ...] = self.weight[..., [1, 2, 3, 0]]\n",
        "        x = torch.nn.functional.conv2d(x, weight.reshape(self.out_channels * 4, self.in_channels * 4, 1, 1), stride=1,\n",
        "                                       padding=0)\n",
        "        return x\n",
        "\n",
        "##################################################\n",
        "class C_4_1x1_(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(C_4_1x1_, self).__init__()\n",
        "        self.net = nn.Conv3d(in_channels, out_channels, 1, bias=True)\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        x = self.net(x.view(b, c // 4, 4, h, w)).reshape(b, self.out_channels * 4, h, w)\n",
        "        return x\n",
        "\n",
        "##################################################\n",
        "class C_4_3x3(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(C_4_3x3, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        weight = torch.randn(out_channels, in_channels, 4, 3, 3) / math.sqrt(4 * in_channels * 9 / 2)\n",
        "        self.weight = torch.nn.Parameter(weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight = torch.zeros(self.out_channels, 4, self.in_channels, 4, 3, 3).to(x.device)\n",
        "        weight[::, 0, ...] = self.weight\n",
        "        weight[::, 1, ...] = torch.rot90(self.weight[..., [3, 0, 1, 2], ::, ::], 1, [3, 4])\n",
        "        weight[::, 2, ...] = torch.rot90(self.weight[..., [2, 3, 0, 1], ::, ::], 2, [3, 4])\n",
        "        weight[::, 3, ...] = torch.rot90(self.weight[..., [1, 2, 3, 0], ::, ::], 3, [3, 4])\n",
        "        x = torch.nn.functional.conv2d(x, weight.reshape(self.out_channels * 4, self.in_channels * 4, 3, 3))\n",
        "        return x\n",
        "\n",
        "##################################################\n",
        "class C_4_BN(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(C_4_BN, self).__init__()\n",
        "        self.bn = nn.BatchNorm3d(in_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        return self.bn(x.reshape(b, c // 4, 4, h, w)).reshape(x.size())\n",
        "\n",
        "##################################################\n",
        "class C_4_Pool(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(C_4_Pool, self).__init__()\n",
        "        self.pool = nn.MaxPool3d((4, 1, 1), (4, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        return self.pool(x.reshape(b, c // 4, 4, h, w)).squeeze(2)\n",
        "\n",
        "##################################################\n",
        "class E4_C4(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 kernel_size,\n",
        "                 reduction_ratio=2,\n",
        "                 groups=1\n",
        "                 ):\n",
        "\n",
        "        super(E4_C4, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "        self.group_channels = groups\n",
        "        self.groups = self.out_channels // self.group_channels\n",
        "        self.dim_g = 4\n",
        "\n",
        "        self.v = nn.Sequential(C_4_1x1(in_channels, out_channels))\n",
        "        self.conv1 = nn.Sequential(C_4_1x1(in_channels, int(in_channels // reduction_ratio)),\n",
        "                                    nn.GroupNorm(int(in_channels // reduction_ratio),int(in_channels // reduction_ratio)*4), nn.ReLU())\n",
        "        self.conv2 = nn.Sequential(C_4_1x1_(int(in_channels // reduction_ratio), kernel_size ** 2 * self.groups))\n",
        "\n",
        "        self.unfold = nn.Unfold(kernel_size, 1, (kernel_size-1)//2, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight = self.conv2(self.conv1(x))\n",
        "        b, c, h, w = weight.shape\n",
        "        weight = weight.view(b, self.groups, self.kernel_size, self.kernel_size, 4, h, w)\n",
        "        weight[::, ::, ::, ::, 1, ::, ::] = torch.rot90(weight[::, ::, ::, ::, 1, ::, ::], 1, [2, 3])\n",
        "        weight[::, ::, ::, ::, 2, ::, ::] = torch.rot90(weight[::, ::, ::, ::, 2, ::, ::], 2, [2, 3])\n",
        "        weight[::, ::, ::, ::, 3, ::, ::] = torch.rot90(weight[::, ::, ::, ::, 3, ::, ::], 3, [2, 3])\n",
        "        weight = weight.reshape(b, self.groups, self.kernel_size ** 2, 4, h, w).unsqueeze(2).transpose(3, 4)\n",
        "        x = self.v(x)\n",
        "        out = self.unfold(x).view(b, self.groups, self.group_channels, 4, self.kernel_size ** 2, h, w)\n",
        "        out = (weight * out).sum(dim=4).view(b, self.out_channels * 4, h, w)\n",
        "        return out\n",
        "\n",
        "##################################################\n",
        "class C_4_Conv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(C_4_Conv, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        weight = torch.randn(out_channels, in_channels, 3, 3) / math.sqrt(9 * in_channels / 2)\n",
        "        self.weight = torch.nn.Parameter(weight)\n",
        "\n",
        "    def forward(self, input):\n",
        "        weight = torch.zeros(self.out_channels, 4, self.in_channels, 3, 3).to(input.device)\n",
        "        weight[::, 0] = self.weight\n",
        "        weight[::, 1] = torch.rot90(self.weight[::], 1, [2, 3])\n",
        "        weight[::, 2] = torch.rot90(self.weight[::], 2, [2, 3])\n",
        "        weight[::, 3] = torch.rot90(self.weight[::], 3, [2, 3])\n",
        "        out = nn.functional.conv2d(input, weight.reshape(self.out_channels * 4, self.in_channels, 3, 3), padding=1)\n",
        "        return out\n",
        "\n",
        "##################################################\n",
        "class E4_net(nn.Module):\n",
        "    def __init__(self, in_channels=1, kernel_size=5, groups=8, reduction_ratio=1, drop=0.2):\n",
        "        super(E4_net, self).__init__()\n",
        "\n",
        "        # lifting convolution\n",
        "        self.conv1=C_4_Conv(in_channels, 16)\n",
        "\n",
        "        # pooling through conv for dimention reductions\n",
        "        self.conv2=E4_C4(16, 16, kernel_size, reduction_ratio=reduction_ratio, groups=groups)\n",
        "        self.conv3=E4_C4(16, 16, kernel_size, reduction_ratio=reduction_ratio, groups=groups)\n",
        "        self.conv4=E4_C4(16, 16, kernel_size, reduction_ratio=reduction_ratio, groups=groups)\n",
        "        self.conv5=E4_C4(16, 16, kernel_size, reduction_ratio=reduction_ratio, groups=groups)\n",
        "        self.conv6=E4_C4(16, 16, kernel_size, reduction_ratio=reduction_ratio, groups=groups)\n",
        "        self.conv7=E4_C4(16, 16, kernel_size, reduction_ratio=reduction_ratio, groups=groups)\n",
        "        self.pool=nn.MaxPool2d(2,2)\n",
        "\n",
        "        self.bn1=C_4_BN(16)\n",
        "        self.bn2=C_4_BN(16)\n",
        "        self.bn3=C_4_BN(16)\n",
        "        self.bn4=C_4_BN(16)\n",
        "        self.bn5=C_4_BN(16)\n",
        "        self.bn6=C_4_BN(16)\n",
        "        self.bn7=C_4_BN(16)\n",
        "\n",
        "        self.drop=nn.Dropout(drop)\n",
        "\n",
        "\n",
        "        self.group_pool=C_4_Pool()\n",
        "        self.global_pool=nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.fully_net = torch.nn.Sequential(\n",
        "            torch.nn.Linear(16, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x=torch.relu(self.conv1(x))\n",
        "        x=torch.relu(self.bn2(self.conv2(x)))\n",
        "        x=self.pool(x)\n",
        "        x=self.drop(torch.relu(self.conv3(x)))\n",
        "        x=self.drop(torch.relu(self.conv4(x)))\n",
        "        x=self.pool(x)\n",
        "        x=self.drop(torch.relu(self.bn5(self.conv5(x))))\n",
        "        x=self.drop(torch.relu(self.conv6(x)))\n",
        "        x=self.drop(torch.relu(self.conv7(x)))\n",
        "        x=self.group_pool(x)\n",
        "        x=self.global_pool(x).reshape(x.size(0),-1)\n",
        "        x=self.fully_net(x)\n",
        "        return x\n",
        "\n",
        "##################################################\n",
        "class C4Basic(nn.Module):\n",
        "    def __init__(self, in_h, in_w, in_channels, kernel, reduction, groups):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1=C_4_Conv(in_channels, 16)\n",
        "\n",
        "        self.forward_function1 = nn.Sequential(\n",
        "            C_4_3x3(16, 16),\n",
        "            C_4_BN(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            C_4_3x3(16, 16),\n",
        "            C_4_BN(16)\n",
        "        )\n",
        "\n",
        "        self.forward_function2 = nn.Sequential(\n",
        "            C_4_3x3(16, 16),\n",
        "            C_4_BN(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            C_4_3x3(16, 4),\n",
        "            C_4_BN(4)\n",
        "        )\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        h = (in_h - 8) // 2 + ( (in_h - 8) % 2 )\n",
        "        w = (in_w - 8) // 2 + ( (in_w - 8) % 2 )\n",
        "\n",
        "        in_nodes = 4 * 4 * h * w\n",
        "        self.fc1 = nn.Linear(in_nodes, 256)\n",
        "        self.fc2 = nn.Linear(256, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = nn.ReLU(inplace=True)(self.forward_function1(x) )\n",
        "        x = nn.ReLU(inplace=True)(self.forward_function2(x) )\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxRDUKF0k_Xe",
        "outputId": "e37c1b96-5614-4b00-a841-85a9770cefae"
      },
      "outputs": [],
      "source": [
        "#tests\n",
        "dataiter = iter(Ciphar.train_loader)\n",
        "images, labels = next(dataiter)\n",
        "images_c4 = images.repeat(1, 4, 1, 1)\n",
        "print(f\"Inputs: images={images.shape}, label={labels.shape}\")\n",
        "\n",
        "TestC4_1x1 = C_4_1x1(3,1)\n",
        "out_a = TestC4_1x1(images_c4)\n",
        "print(f\"C_4_1x1 outputs: {out_a.shape}\")\n",
        "\n",
        "TestC4_1x1_ = C_4_1x1_(3,2)\n",
        "out_b = TestC4_1x1_(images_c4)\n",
        "print(f\"C_4_1x1_ outputs: {out_b.shape}\")\n",
        "\n",
        "TestC4_3x3 = C_4_3x3(3,3)\n",
        "out_c = TestC4_3x3(images_c4)\n",
        "print(f\"TestC4_3x3 outputs: {out_c.shape}\")\n",
        "\n",
        "TestEC4 = E4_C4(3,4,5, reduction_ratio=2, groups=1)\n",
        "out_d = TestEC4(images_c4)\n",
        "print(f\"TestEC4 outputs: {out_d.shape}\")\n",
        "\n",
        "TestC4Basic = C4Basic(\n",
        "                      in_h=32,\n",
        "                      in_w=32,\n",
        "                      in_channels=3,\n",
        "                      kernel=5,\n",
        "                      reduction=1,\n",
        "                      groups=1)\n",
        "out_e = TestC4Basic(images)\n",
        "print(f\"TestC4Basic outputs: {out_e.shape}\")\n",
        "\n",
        "TestE4_net = E4_net(in_channels=3,kernel_size=5, groups=8, reduction_ratio=1, drop=0.2)\n",
        "out_f = TestE4_net(images)\n",
        "print(f\"TestE4_net outputs: {out_f.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8yyqRLGG4_Z"
      },
      "source": [
        "Basic Equivarient Network Trained without equivariance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_ppOSpXGk3K",
        "outputId": "2695a0a9-6c70-4f29-fb3c-6dad89fd34e5"
      },
      "outputs": [],
      "source": [
        "# loss function\n",
        "import torch.optim as optim\n",
        "C4BasicNet = C4Basic(\n",
        "                      in_h=28,\n",
        "                      in_w=28,\n",
        "                      in_channels=1,\n",
        "                      kernel=5,\n",
        "                      reduction=2,\n",
        "                      groups=1).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(C4BasicNet.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# print total number of parameters\n",
        "par = sum(p.numel() for p in C4BasicNet.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters of BaselineCNN: {par}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yWP53ii923S"
      },
      "outputs": [],
      "source": [
        "C4BasicNet_mnist = Trainer(C4BasicNet, criterion, optimizer, Mnist.train_loader, Mnist.test_loader)\n",
        "print(\"E4_net no equivarience\")\n",
        "C4BasicNet_mnist.train(1)\n",
        "C4BasicNet_mnist.test()\n",
        "C4BasicNet_mnist.test_c4()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad_ZJdaAHB_j"
      },
      "source": [
        "Basic Equivarient Network trained with equivariant datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8AzqFfkHBb3",
        "outputId": "3fcd72ce-b100-4c0d-b629-eca3b1184b43"
      },
      "outputs": [],
      "source": [
        "# loss function\n",
        "import torch.optim as optim\n",
        "C4BasicNet = C4Basic(\n",
        "                      in_h=28,\n",
        "                      in_w=28,\n",
        "                      in_channels=1,\n",
        "                      kernel=5,\n",
        "                      reduction=2,\n",
        "                      groups=1).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(C4BasicNet.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# print total number of parameters\n",
        "par = sum(p.numel() for p in C4BasicNet.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters of BaselineCNN: {par}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIHD6ambHBmy"
      },
      "outputs": [],
      "source": [
        "C4BasicNet_mnist = Trainer(C4BasicNet, criterion, optimizer, Mnist.train_loader, Mnist.test_loader)\n",
        "print(\"E4_net no equivarience\")\n",
        "C4BasicNet_mnist.train_c4(1)\n",
        "C4BasicNet_mnist.test()\n",
        "C4BasicNet_mnist.test_c4()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPUpHWv6nD-g",
        "outputId": "7c32e930-c8e9-411e-9d64-3d3b74249473"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "TestE4_net = E4_net(kernel_size=5, groups=8, reduction_ratio=1, drop=0.2).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(TestE4_net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# print total number of parameters\n",
        "par = sum(p.numel() for p in TestE4_net.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters of E4_net: {par}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7o4V-wafGQyp"
      },
      "outputs": [],
      "source": [
        "E4_net_mnist = Trainer(TestE4_net, criterion, optimizer, Mnist.train_loader, Mnist.test_loader)\n",
        "print(\"E4_net no equivarience\")\n",
        "E4_net_mnist.train(1)\n",
        "E4_net_mnist.test()\n",
        "E4_net_mnist.test_c4()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRoSeu1avjlY"
      },
      "outputs": [],
      "source": [
        "E4_net_mnist = Trainer(TestE4_net, criterion, optimizer, Mnist.train_loader, Mnist.test_loader)\n",
        "print(\"E4_net no equivarience\")\n",
        "E4_net_mnist.train_c4(1)\n",
        "E4_net_mnist.test()\n",
        "E4_net_mnist.test_c4()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3BlE69tK5mv"
      },
      "source": [
        "## Network 3. Efficient Group Equivarient CNN with Equivarient Maxpooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Bb99L_NLjdc6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "class EquivariantSubsample(torch.nn.Module):\n",
        "  def __init__(self, reduction=(2,2)):\n",
        "    super().__init__()\n",
        "    self.width_reduction = reduction[0]\n",
        "    self.height_reduction = reduction[1]\n",
        "\n",
        "  def max(self, sample):\n",
        "    max_values, max_w_indices = torch.max(sample, dim=3)\n",
        "    max_values, h = torch.max(max_values, dim=2)\n",
        "    w = torch.gather(max_w_indices, dim=2, index=h.unsqueeze(-1)).squeeze(-1)\n",
        "    max_indices = torch.stack((h, w), dim=-1)\n",
        "    return max_values, max_indices\n",
        "\n",
        "  def get_p(self, image):\n",
        "    b, c, h, w = image.shape\n",
        "    max_values, max_index = self.max(image)\n",
        "    max_h_idx = max_index[::, ::, 0] % self.height_reduction\n",
        "    max_w_idx = max_index[::, ::, 1] % self.width_reduction\n",
        "    return max_h_idx, max_w_idx\n",
        "\n",
        "  def block_pool(self, img, y_offsets, x_offsets):\n",
        "    batch_size, channels, height, width = img.shape\n",
        "    h = self.height_reduction\n",
        "    w = self.width_reduction\n",
        "\n",
        "    # Clamp to ensure offsets are within bounds for the pool_size\n",
        "    y_offsets = torch.clamp(y_offsets, 0, height - h)\n",
        "    x_offsets = torch.clamp(x_offsets, 0, width - w)\n",
        "\n",
        "    # extend to batch and channel\n",
        "    y_offsets = y_offsets.unsqueeze(-1).unsqueeze(-1).repeat(1,1,h,w).to(device)\n",
        "    x_offsets = x_offsets.unsqueeze(-1).unsqueeze(-1).repeat(1,1,h,w).to(device)\n",
        "\n",
        "    # Generate grid for h x w window\n",
        "    y_relative = torch.arange(h).view(1, 1, h, 1).repeat(batch_size, channels, 1, w).to(device)\n",
        "    x_relative = torch.arange(w).view(1, 1, 1, w).repeat(batch_size, channels, h,1).to(device)\n",
        "\n",
        "    # Calculate absolute indices for y and x within the h x w block\n",
        "    y_indices = y_offsets + y_relative\n",
        "    x_indices = x_offsets + x_relative\n",
        "\n",
        "    block = img[torch.arange(batch_size).view(-1, 1, 1, 1),\n",
        "                torch.arange(channels).view(1, -1, 1, 1),\n",
        "                y_indices, x_indices]\n",
        "    block, indexes = self.max(block)\n",
        "    return block.to(device)\n",
        "\n",
        "  def forward(self, images, p):\n",
        "    b, c, h, w = images.shape\n",
        "    p_w = p[0].unsqueeze(-1).to(device)\n",
        "    p_h = p[1].unsqueeze(-1).to(device)\n",
        "\n",
        "    w_sample_indices = p_w + torch.arange(0, w, self.width_reduction).to(device)\n",
        "    h_sample_indices = p_h + torch.arange(0, h, self.height_reduction).to(device)\n",
        "\n",
        "    out_h = h//self.height_reduction\n",
        "    out_w = w//self.width_reduction\n",
        "\n",
        "    output = torch.zeros(b,c,out_h, out_w).to(device)\n",
        "    for i in range(out_h):\n",
        "      for j in range(out_w):\n",
        "        y_offsets = h_sample_indices[::,::,i]\n",
        "        x_offsets = w_sample_indices[::,::,j]\n",
        "        output[::,::, i,j] = self.block_pool(images, y_offsets, x_offsets)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IgwwgCAe89E",
        "outputId": "77704f92-ee16-4e90-e96a-d41ae3667463"
      },
      "outputs": [],
      "source": [
        "#test equivariant function\n",
        "sub = EquivariantSubsample(reduction=(2,2))\n",
        "test_image = torch.tensor([[\n",
        "                [15,2,3,4,5],\n",
        "                [6,7,8,9,10],\n",
        "                [11,12,13,14,15],\n",
        "                [16,17,18,19,20],\n",
        "                [21,22,23,24,25]\n",
        "              ],\n",
        "\n",
        "                [[1,2,3,4,5],\n",
        "                [6,7,8,9,10],\n",
        "                [11,12,13,14,15],\n",
        "                [16,17,2,25,20],\n",
        "                [21,22,22,24,23]],\n",
        "\n",
        "              [[1,2,3,4,5],\n",
        "                [6,7,8,9,10],\n",
        "                [11,12,13,14,15],\n",
        "                [16,17,18,19,20],\n",
        "                [21,22,23,25,24]\n",
        "              ]\n",
        "              ]).repeat(4,1,1,1).to(device)\n",
        "print(test_image)\n",
        "print(test_image.shape)\n",
        "m, indices = sub.max(test_image)\n",
        "p = sub.get_p(test_image)\n",
        "print(f\"max height: {p[0]}, max width: {p[1]}\")\n",
        "print(m)\n",
        "print(indices)\n",
        "\n",
        "o = sub.forward(test_image, p)\n",
        "print(f\"Outut: {o}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbZI97vrINCv"
      },
      "source": [
        "Equivarient Network With subsampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oq5n9XvcIMWZ"
      },
      "outputs": [],
      "source": [
        "class E4_Pooling(nn.Module):\n",
        "    def __init__(self, in_w, in_channels=1, kernel_size=5, groups=8, reduction_ratio=1, drop=0.2):\n",
        "        super(E4_Pooling, self).__init__()\n",
        "\n",
        "        # lifting convolution\n",
        "        self.liftingConv=C_4_Conv(in_channels, 16)\n",
        "\n",
        "        # conv 3x3 + equivariant pooling\n",
        "        self.conv3x3_1 = C_4_3x3(16,16)\n",
        "        self.pool1 = EquivariantSubsample(reduction=(2,2))\n",
        "        self.conv3x3_2 = C_4_3x3(16,16)\n",
        "        self.pool2 = EquivariantSubsample(reduction=(2,2))\n",
        "\n",
        "        # E4 network\n",
        "        self.conv1=E4_C4(16, 16, kernel_size, reduction_ratio=reduction_ratio, groups=groups)\n",
        "        self.conv2=E4_C4(16, 16, kernel_size, reduction_ratio=reduction_ratio, groups=groups)\n",
        "        self.conv3=E4_C4(16, 16, kernel_size, reduction_ratio=1, groups=groups)\n",
        "\n",
        "        self.ReductionConv1=E4_C4(4, 2, kernel_size, reduction_ratio=1, groups=1)\n",
        "        self.ReductionConv2=E4_C4(2, 1, kernel_size, reduction_ratio=1, groups=1)\n",
        "\n",
        "        # norm functions\n",
        "        self.bn1=C_4_BN(16)\n",
        "        self.bn2=C_4_BN(16)\n",
        "        self.bn3=C_4_BN(16)\n",
        "\n",
        "        self.group_pool=C_4_Pool()\n",
        "        #self.global_pool=nn.AdaptiveMaxPool2d()\n",
        "        final_size = ((((in_w-2)//2 ) - 2) // 2)**2\n",
        "        self.fully_net = torch.nn.Sequential(\n",
        "            torch.nn.Linear(final_size*4, 36),\n",
        "            torch.nn.Linear(36, 10)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 3x3 conv 1\n",
        "        x=torch.relu(self.liftingConv(x))\n",
        "        x = self.conv3x3_1(x)\n",
        "        p1 = self.pool1.get_p(x)\n",
        "        x = self.pool1(x, p1)\n",
        "\n",
        "        # 3x3 conv 2\n",
        "        x = self.conv3x3_2(x)\n",
        "        p2 = self.pool2.get_p(x)\n",
        "        x = self.pool1(x,p2)\n",
        "\n",
        "\n",
        "        # x=torch.relu(self.bn2(self.conv2(x)))\n",
        "        # x=self.pool(x)\n",
        "\n",
        "        x=torch.relu(self.bn1(self.conv1(x)))\n",
        "        x=torch.relu(self.bn2(self.conv2(x)))\n",
        "        x=torch.relu(self.bn3(self.conv3(x)))\n",
        "\n",
        "        #x = self.ReductionConv2(x)\n",
        "\n",
        "        x=self.group_pool(x)\n",
        "        x = self.ReductionConv1(x)\n",
        "        x = self.ReductionConv2(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x=self.fully_net(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRXc2BSe2PUm",
        "outputId": "7c205a14-9e2c-43e1-f627-558ecf28f858"
      },
      "outputs": [],
      "source": [
        "# loss function\n",
        "import torch.optim as optim\n",
        "E4_Pooling_net = E4_Pooling(in_w=32, in_channels=3, kernel_size=5, groups=8, reduction_ratio=1, drop=0.2).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(E4_Pooling_net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# print total number of parameters\n",
        "par = sum(p.numel() for p in E4_Pooling_net.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters of ES4_Net: {par}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Eq3MUPn5Ejx",
        "outputId": "958b942c-b319-40b5-f14a-ab185529045d"
      },
      "outputs": [],
      "source": [
        "dataiter = iter(Ciphar.train_loader)\n",
        "images, labels = next(dataiter)\n",
        "images, lables = images.to(device), labels.to(device)\n",
        "images_c4 = images.repeat(1, 4, 1, 1)\n",
        "print(f\"Inputs: images={images.shape}, label={labels.shape}\")\n",
        "out_a =E4_Pooling_net(images)\n",
        "print(f\"Final output: {out_a.shape}\")\n",
        "print(out_a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OXO7vKG_Qyn",
        "outputId": "2dda4d7e-1a49-4815-d9d3-db66ea243825"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "E4_Pooling_net = E4_Pooling(in_w=28,in_channels=1, kernel_size=5, groups=8, reduction_ratio=1, drop=0.2).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(E4_Pooling_net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# print total number of parameters\n",
        "par = sum(p.numel() for p in E4_Pooling_net.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters of E4_Pooling_net: {par}\")\n",
        "\n",
        "E4_Pooling_net_mnist = Trainer(E4_Pooling_net, criterion, optimizer, Mnist.train_loader, Mnist.test_loader)\n",
        "print(\"E4_net no equivarience\")\n",
        "E4_Pooling_net_mnist.train(2)\n",
        "E4_Pooling_net_mnist.test()\n",
        "E4_Pooling_net_mnist.test_c4()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pPrhpTXrkrqk",
        "outputId": "b8ecba70-62fe-4c91-c107-b2d60941f159"
      },
      "outputs": [],
      "source": [
        "E4_Pooling_net = E4_Pooling(in_w=28,in_channels=1, kernel_size=5, groups=8, reduction_ratio=1, drop=0.2).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(E4_Pooling_net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# print total number of parameters\n",
        "par = sum(p.numel() for p in E4_Pooling_net.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters of E4_Pooling_net: {par}\")\n",
        "\n",
        "E4_Pooling_net_mnist = Trainer(E4_Pooling_net, criterion, optimizer, Mnist.train_loader, Mnist.test_loader)\n",
        "print(\"E4_net no equivarience\")\n",
        "E4_Pooling_net_mnist.train_c4(2)\n",
        "E4_Pooling_net_mnist.test()\n",
        "E4_Pooling_net_mnist.test_c4()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8FjlFU6VGlN",
        "outputId": "c9eabc73-f4ef-4aa0-e325-a165649afd99"
      },
      "outputs": [],
      "source": [
        "E4_Pooling_net_mnist.test()\n",
        "E4_Pooling_net_mnist.test_c4()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAKiQdOaK1ND"
      },
      "source": [
        "## Network 4. Efficient Group Equivarient CNN with Equivarient Maxpooling and Non-equivarient Attention Layers"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
